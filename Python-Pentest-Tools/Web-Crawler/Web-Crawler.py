#!/usr/bin/env python

import requests
import time
import sys
import re

def main():
    #
    urls = []
    #
    urls_subordinate = []
    #
    print("""
***********************************
*  ___      _   _                 * 
* | _ \_  _| |_| |_  ___ _ _      * 
* |  _/ || |  _| ' \/ _ \ ' \     * 
* |_|  \_, |\__|_||_\___/_||_|    * 
* __   |__/_   _                  * 
* \ \    / /__| |__               * 
*  \ \/\/ / -_) '_ \              * 
*   \_/\_/\___|_.__/    _         *
*  / __|_ _ __ ___ __ _| |___ _ _ * 
* | (__| '_/ _` \ V  V / / -_) '_|*
*  \___|_| \__,_|\_/\_/|_\___|_|  *
*                                 *
***********************************
          """)
    #
    time.sleep(1)
    #
    print()
    #
    crawledSites = [] ; remainingSites = []
    #
    counter = int(input("[+] Enter the counter delimiter (integer limit to prevent endless crawling)-> "))
    #
    targetUrl = input("[+] Enter the target URL-> ")
    #
    failureIndex = 0
    #
    request = requests.get(targetUrl)
    #
    requestContent = request.content
    #
    requestString = str(request)
    #
    if('2' in requestString):
        #
        remainingSites.append(targetUrl)
        #
    else:
        #
        sys.exit("[!] Site couldn't be reached, check input formatting [!]")
        #
    while(remainingSites or failureIndex < counter):
        #
        currentUrl = remainingSites.pop(0)
        #
        request = requests.get(currentUrl)
        #
        requestContent = request.content
        #
        requestString = str(request)
        #
        if('4' in requestString):
            #
            failureIndex += 1
            #
        for url in re.findall('<a href="([^"]+)">',str(requestContent)):
            #
            print("[*] Found: %s " % url)
            #
            if(url[0] == '/'):
                #
                url = currentUrl + url
                #
            p = re.compile('https?')
            #
            if(p.match(url)):
                #
                remainingSites.append(url)
                #
            crawledSites.append(url)
            #
            time.sleep(1)
        



if(__name__ == '__main__'):
    #
    main()